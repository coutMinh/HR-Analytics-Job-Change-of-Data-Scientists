# HR Analytics: Job Change Prediction of Data Scientists

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.8+-blue.svg" alt="Python">
  <img src="https://img.shields.io/badge/NumPy-1.26.4-orange.svg" alt="NumPy">
  <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License: MIT">
  <img src="https://img.shields.io/badge/Dataset_License-GPL--3.0-blue.svg" alt="Dataset License: GPL-3.0">
</p>

---

## M·ª•c l·ª•c

- [Gi·ªõi thi·ªáu](#-gi·ªõi-thi·ªáu)
- [Dataset](#-dataset)
- [Ph∆∞∆°ng ph√°p](#-ph∆∞∆°ng-ph√°p)
- [C√†i ƒë·∫∑t](#-c√†i-ƒë·∫∑t)
- [S·ª≠ d·ª•ng](#-s·ª≠-d·ª•ng)
- [K·∫øt qu·∫£](#-k·∫øt-qu·∫£)
- [C·∫•u tr√∫c d·ª± √°n](#-c·∫•u-tr√∫c-d·ª±-√°n)
- [Th√°ch th·ª©c & Gi·∫£i ph√°p](#-th√°ch-th·ª©c--gi·∫£i-ph√°p)
- [H∆∞·ªõng ph√°t tri·ªÉn](#-h∆∞·ªõng-ph√°t-tri·ªÉn)
- [Contributors](#-contributors)
- [License](#-license)

---

## Gi·ªõi thi·ªáu

### M√¥ t·∫£ b√†i to√°n

B√†i to√°n d·ª± ƒëo√°n li·ªáu m·ªôt ·ª©ng vi√™n c√≥ ƒëang t√¨m ki·∫øm vi·ªác l√†m m·ªõi hay s·∫Ω ti·∫øp t·ª•c l√†m vi·ªác t·∫°i c√¥ng ty hi·ªán t·∫°i sau khi ho√†n th√†nh m·ªôt kh√≥a ƒë√†o t·∫°o. ƒê√¢y l√† b√†i to√°n **binary classification** v·ªõi:
- **Class 0**: Kh√¥ng t√¨m ki·∫øm c√¥ng vi·ªác m·ªõi (75%)
- **Class 1**: ƒêang t√¨m ki·∫øm c√¥ng vi·ªác m·ªõi (25%)

### ƒê·ªông l·ª±c v√† ·ª©ng d·ª•ng th·ª±c t·∫ø

**T·∫°i sao b√†i to√°n n√†y quan tr·ªçng?**
1. **T·ªëi ∆∞u chi ph√≠ tuy·ªÉn d·ª•ng**: C√¥ng ty c√≥ th·ªÉ t·∫≠p trung ngu·ªìn l·ª±c v√†o nh·ªØng ·ª©ng vi√™n th·ª±c s·ª± c√≥ √Ω ƒë·ªãnh gia nh·∫≠p
2. **C·∫£i thi·ªán chi·∫øn l∆∞·ª£c ƒë√†o t·∫°o**: Hi·ªÉu r√µ y·∫øu t·ªë n√†o ·∫£nh h∆∞·ªüng ƒë·∫øn quy·∫øt ƒë·ªãnh c·ªßa ·ª©ng vi√™n
3. **Gi·∫£m t·ª∑ l·ªá churn nh√¢n s·ª±**: D·ª± ƒëo√°n s·ªõm ƒë·ªÉ c√≥ k·∫ø ho·∫°ch gi·ªØ ch√¢n nh√¢n t√†i

**·ª®ng d·ª•ng th·ª±c t·∫ø:**
- HR Analytics: T·ª± ƒë·ªông h√≥a quy tr√¨nh s√†ng l·ªçc h·ªì s∆°
- Edtech platforms: ƒê√°nh gi√° hi·ªáu qu·∫£ kh√≥a h·ªçc d·ª±a tr√™n m·ª•c ti√™u ngh·ªÅ nghi·ªáp c·ªßa h·ªçc vi√™n
- Recruitment agencies: Matching ·ª©ng vi√™n v·ªõi c√¥ng ty ph√π h·ª£p

### M·ª•c ti√™u c·ª• th·ªÉ

1. **X√¢y d·ª±ng m√¥ h√¨nh ho√†n ch·ªânh**: Implement Logistic Regression v√† to√†n b·ªô pipeline 
2. **X·ª≠ l√Ω d·ªØ li·ªáu ph·ª©c t·∫°p**: Handling missing values, categorical encoding, feature engineering
3. **X·ª≠ l√Ω imbalanced data**: S·ª≠ d·ª•ng class weights ƒë·ªÉ c√¢n b·∫±ng
5. **Visualization**: Ph√¢n t√≠ch EDA, ƒë·∫∑t c√¢u h·ªèi ƒë·ªÉ c√≥ insights

---

## Dataset

### Ngu·ªìn d·ªØ li·ªáu

Dataset t·ª´ cu·ªôc thi **HR Analytics Job Change of Data Scientists** v·ªõi th√¥ng tin c·ªßa ·ª©ng vi√™n ƒë√£ tham gia c√°c kh√≥a ƒë√†o t·∫°o.

- **Training set**: 19,158 samples
- **Test set**: 2,129 samples
- **Features**: 14 c·ªôt (13 features + 1 target)

### M√¥ t·∫£ c√°c features

| Feature | Ki·ªÉu | M√¥ t·∫£ | Missing % |
|---------|------|-------|-----------|
| `enrollee_id` | Categorical | ID duy nh·∫•t c·ªßa ·ª©ng vi√™n | 0% |
| `city` | Categorical | M√£ th√†nh ph·ªë (123 unique) | 0% |
| `city_development_index` | Numeric | Ch·ªâ s·ªë ph√°t tri·ªÉn th√†nh ph·ªë (0-1) | 0% |
| `gender` | Categorical | Gi·ªõi t√≠nh (Male/Female/Other) | 23.5% |
| `relevent_experience` | Binary | C√≥ kinh nghi·ªám li√™n quan (Yes/No) | 0% |
| `enrolled_university` | Categorical | Lo·∫°i kh√≥a h·ªçc ƒë·∫°i h·ªçc ƒëang theo h·ªçc | 2.0% |
| `education_level` | Categorical | Tr√¨nh ƒë·ªô h·ªçc v·∫•n | 2.4% |
| `major_discipline` | Categorical | Chuy√™n ng√†nh | 14.7% |
| `experience` | Ordinal | T·ªïng s·ªë nƒÉm kinh nghi·ªám | 0.3% |
| `company_size` | Ordinal | Quy m√¥ c√¥ng ty hi·ªán t·∫°i | 30.9% |
| `company_type` | Categorical | Lo·∫°i h√¨nh c√¥ng ty | 32.0% |
| `last_new_job` | Ordinal | Kho·∫£ng th·ªùi gian c√¥ng vi·ªác tr∆∞·ªõc | 2.2% |
| `training_hours` | Numeric | S·ªë gi·ªù ƒë√†o t·∫°o ho√†n th√†nh | 0% |
| `target` | Binary | 0 = Kh√¥ng ƒë·ªïi vi·ªác, 1 = ƒê·ªïi vi·ªác | 0% |

### K√≠ch th∆∞·ªõc v√† ƒë·∫∑c ƒëi·ªÉm d·ªØ li·ªáu

**ƒê·∫∑c ƒëi·ªÉm ch√≠nh:**
- **Class Imbalance**: T·ª∑ l·ªá 3:1 (75% class 0 vs 25% class 1)
- **Missing Values**: 6/14 features c√≥ gi√° tr·ªã thi·∫øu (2% - 32%)
- **Mixed Data Types**: Numeric (2), Binary (1), Ordinal (3), Categorical (8)
- **High Cardinality**: Feature `city` c√≥ 123 unique values


---

## Ph∆∞∆°ng ph√°p

### Quy tr√¨nh x·ª≠ l√Ω d·ªØ li·ªáu

#### 1. **Missing Values Handling**

Chi·∫øn l∆∞·ª£c x·ª≠ l√Ω theo t·ª∑ l·ªá missing:

```python
# Nh√≥m thi·∫øu nhi·ªÅu (>20%): Fill with -1 ho·∫∑c "Unknown"
if missing_pct > 20:
    fill_value = -1  # Cho ordinal encoding
    
# Nh√≥m thi·∫øu √≠t (<15%): Fill with Mode
elif missing_pct < 15:
    fill_value = mode_value
```

#### 2. **Feature Engineering**

**Standardization (Z-score normalization):**

$$
z = \frac{x - \mu}{\sigma}
$$

√Åp d·ª•ng cho: `city_development_index`, `training_hours`

```python
def standardize(values, mean=None, std=None, fit=True):
    valid = values != missing_value
    if fit:
        mean = np.mean(values[valid])
        std = np.std(values[valid])
    result = np.zeros_like(values)
    result[valid] = (values[valid] - mean) / std
    return result, mean, std
```

**Binary Encoding:**
- `relevent_experience`: Has relevant = 1, No relevant = 0

**Ordinal Encoding:**
- `experience`: 5 bins (<1, 1-2, 3-5, 6-15, >15)
- `company_size`: 8 levels (<10, 10-49, 50-99, ..., 10000+)
- `last_new_job`: 6 levels (never, 1, 2, 3, 4, >4)

**One-Hot Encoding:**
- `enrolled_university`: 3 dummies
- `education_level`: 5 dummies
- `major_discipline`: 6 dummies
- `company_type`: 6 dummies

```python
def one_hot(values, categories=None, fit=True):
    if fit:
        categories = sorted(set(values))
    encoded = np.zeros((len(values), len(categories)))
    for i, v in enumerate(values):
        if v in categories:
            encoded[i, categories.index(v)] = 1
    return encoded, categories
```

**Interaction Features:**
- `experience_bin √ó relevent_experience`: T∆∞∆°ng t√°c gi·ªØa kinh nghi·ªám v√† ƒë·ªô li√™n quan
- `company_size √ó last_new_job`: T∆∞∆°ng t√°c quy m√¥ c√¥ng ty v√† t√≠nh ·ªïn ƒë·ªãnh

**Missing Indicators:**
- Binary flags cho c√°c features c√≥ missing values

**K·∫øt qu·∫£:** 14 features g·ªëc ‚Üí **39 features** sau preprocessing

#### 3. **Features lo·∫°i b·ªè**
- `enrollee_id`: ID kh√¥ng c√≥ √Ω nghƒ©a d·ª± ƒëo√°n
- `city`: Qu√° nhi·ªÅu categories (123), thay b·∫±ng `city_development_index`

---

### Thu·∫≠t to√°n: Logistic Regression v·ªõi Gradient Descent

#### C√¥ng th·ª©c to√°n h·ªçc

**1. Sigmoid:**

$$
h_\theta(x) = \sigma(z) = \frac{1}{1 + e^{-z}}
$$

$$
z = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n = \theta^T x
$$
- V·ªõi $x_i$ l√† c√°c feature ƒë∆∞·ª£c s·ª≠ d·ª•ng cho b√†i to√°n ph√¢n lo·∫°i

**2. Cost Function (Log Loss):**

**Binary Cross-Entropy:**

$$
J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right]
$$

**Normal cost function with class weight and L2 regularization (imbalance dataset):**

$$
J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} w_i \left[ y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right] + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2
$$

Trong ƒë√≥:
- $m$: s·ªë l∆∞·ª£ng samples
- $y^{(i)}$: label th·ª±c t·∫ø (0 ho·∫∑c 1) c·ªßa sample th·ª© $i$
- $h_\theta(x^{(i)})$: x√°c su·∫•t d·ª± ƒëo√°n cho sample th·ª© $i$
- $w_i$: tr·ªçng s·ªë c·ªßa sample th·ª© $i$ (ƒë·ªÉ x·ª≠ l√Ω class imbalance)
- $\lambda$: h·ªá s·ªë regularization (L2, gi·∫£m overfitting)
- $n$: s·ªë l∆∞·ª£ng features

**3. Gradient:**


$$
\frac{\partial J}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right) x_j^{(i)}
$$


**4. Gradient Descent Update:**

$$
\theta_j := \theta_j - \alpha \frac{\partial J}{\partial \theta_j}
$$

Trong ƒë√≥ $\alpha$ l√† learning rate.

**5. Class Weights (x·ª≠ l√Ω imbalanced data):**

$$
w_{\text{class}} = \frac{n_{\text{samples}}}{n_{\text{classes}} \times n_{\text{samples in class}}}
$$

- Class 0 (75%): weight ‚âà 0.67
- Class 1 (25%): weight ‚âà 2.0

---

### Implementation b·∫±ng NumPy

#### **Sigmoid Function**

```python
def _sigmoid(z: np.ndarray) -> np.ndarray:
    return 1.0 / (1.0 + np.exp(z))
```

#### **Forward Propagation (Vectorized)**

```python
def predict_proba(self, X: np.ndarray) -> np.ndarray:
    # Th√™m bias term 
    X_aug = np.hstack((np.ones((X.shape[0], 1)), X))
    
    # T√≠nh z = X @ weights
    logits = np.einsum("ij,j->i", X_aug, self.weights)
    
    # √Åp d·ª•ng sigmoid
    return _sigmoid(logits)
```



#### **Train/Test Split**

```python
def train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42):
    n_samples = X.shape[0]
    indices = np.arange(n_samples)
    
    if shuffle:
        rng = np.random.default_rng(random_state)
        rng.shuffle(indices)
    
    split_idx = int(n_samples * (1 - test_size))
    train_idx, test_idx = indices[:split_idx], indices[split_idx:]
    
    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]
```

---

## C√†i ƒë·∫∑t

### Y√™u c·∫ßu h·ªá th·ªëng

- Python 3.8+
- NumPy 1.26.4
- Matplotlib 3.8.2
- Seaborn 0.13.0

### C√°c b∆∞·ªõc c√†i ƒë·∫∑t

**1. Clone repository:**

```bash
git clone <repo>
cd "HR Analytics"
```

**2. C√†i ƒë·∫∑t dependencies:**

```bash
pip install -r requirements.txt
```

**File `requirements.txt`:**
```
numpy==1.26.4
matplotlib==3.8.2
seaborn==0.13.0
```

---

## S·ª≠ d·ª•ng

### Workflow ho√†n ch·ªânh

#### **Step 1: Exploratory Data Analysis (EDA)**

```bash
# M·ªü Jupyter notebook ho·∫∑c ch·∫°y tr·ª±c ti·∫øp
jupyter notebook notebooks/01_data_exploration.ipynb
```

**Ho·∫∑c ch·∫°y t·ª´ng ph·∫ßn trong notebook:**
- Load v√† kh√°m ph√° d·ªØ li·ªáu
- Ph√¢n t√≠ch missing values
- Tr·ª±c quan h√≥a m·ªëi quan h·ªá gi·ªØa features v√† target
- Ph√¢n t√≠ch correlation

#### **Step 2: Data Preprocessing**

```bash
jupyter notebook notebooks/02_preprocessing.ipynb
```
**Ho·∫∑c ch·∫°y t·ª´ng ph·∫ßn trong notebook (recommend)**

**Output:**
- `data/processed/train_processed.csv`: 19,158 samples √ó 40 features
- `data/processed/test_processed.csv`: 2,129 samples √ó 39 features
- `data/processed/artifacts.json`: Scaling parameters v√† categories

#### **Step 3: Model Training & Evaluation**

```bash
jupyter notebook notebooks/03_modeling.ipynb
```
**Ho·∫∑c ch·∫°y t·ª´ng ph·∫ßn trong notebook (recommend)**


---

## K·∫øt qu·∫£

### Metrics ƒë·∫°t ƒë∆∞·ª£c

| Model | Accuracy | Precision | Recall | 
|-------|----------|-----------|--------|
| **Baseline** (No weights) |  0.7740   | 0.5645 | 0.2880 | 
| **Balanced** (With weights) | 0.7213 | 0.4524 | 0.7228 | 

### Visualizations


![Training Loss](output1.png)
*H√¨nh 1: Loss gi·∫£m d·∫ßn v√† h·ªôi t·ª• sau ~200 epochs*

![Training Accuracy](output2.png)
*H√¨nh 2: Accuracy ·ªïn ƒë·ªãnh quanh 75-78%*

---

## üìÅ C·∫•u tr√∫c d·ª± √°n

```
HR Analytics/
‚îÇ
‚îú‚îÄ‚îÄ README.md                
‚îú‚îÄ‚îÄ requirements.txt          # Dependencies
‚îÇ
‚îú‚îÄ‚îÄ data/                     # D·ªØ li·ªáu
‚îÇ   ‚îú‚îÄ‚îÄ raw/                  # D·ªØ li·ªáu g·ªëc
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aug_train.csv     
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aug_test.csv      
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sample_submission.csv
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ processed/            # D·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
‚îÇ       ‚îú‚îÄ‚îÄ train_processed.csv  
‚îÇ       ‚îú‚îÄ‚îÄ test_processed.csv   
‚îÇ       ‚îî‚îÄ‚îÄ artifacts.json        # Scaling params & categories
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                # Jupyter notebooks
‚îÇ   ‚îú‚îÄ‚îÄ 01_data_exploration.ipynb   # EDA, visualizations, insights
‚îÇ   ‚îú‚îÄ‚îÄ 02_preprocessing.ipynb      # Data cleaning & feature engineering
‚îÇ   ‚îî‚îÄ‚îÄ 03_modeling.ipynb           # Training, evaluation, comparison
‚îÇ
‚îú‚îÄ‚îÄ src/                      # Source code 
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ data_processing.py    # Preprocessing pipeline  
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models.py             # ML models 
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ visualization.py      # Plotting functions 
‚îÇ
‚îî‚îÄ‚îÄ outputs/                  # K·∫øt qu·∫£ d·ª± ƒëo√°n
    ‚îî‚îÄ‚îÄ submission.csv # Predictions cho test set
```
---

## Th√°ch th·ª©c & Gi·∫£i ph√°p

### Th√°ch th·ª©c 1: Kh√¥ng c√≥ Pandas

**V·∫•n ƒë·ªÅ:**
- Kh√¥ng c√≥ `pd.read_csv()`, `pd.fillna()`, `pd.get_dummies()`
- Ph·∫£i x·ª≠ l√Ω missing values v√† categorical encoding th·ªß c√¥ng

**Gi·∫£i ph√°p:**
```python
# ƒê·ªçc CSV v·ªõi NumPy
data = np.genfromtxt(filepath, delimiter=',', skip_header=1, 
                     dtype=str, encoding='utf-8')
with open(filepath, 'r') as f:
    headers = f.readline().strip().split(',')

# One-hot encoding th·ªß c√¥ng
def one_hot(values, categories=None, fit=True):
    if fit:
        categories = sorted(set(values))
    encoded = np.zeros((len(values), len(categories)))
    for i, v in enumerate(values):
        if v in categories:
            encoded[i, categories.index(v)] = 1
    return encoded, categories
```

### Th√°ch th·ª©c 2: Kh√¥ng c√≥ Scikit-learn

**V·∫•n ƒë·ªÅ:**
- Kh√¥ng c√≥ `LogisticRegression()`, `train_test_split()`, `classification_report()`
- Ph·∫£i implement t·ª´ ƒë·∫ßu to√†n b·ªô ML pipeline


### Th√°ch th·ª©c 3: Class Imbalance (75% vs 25%)

**V·∫•n ƒë·ªÅ:**
- Model bias v·ªÅ class 0 (majority class)
- Precision cao nh∆∞ng Recall th·∫•p cho class 1

**Gi·∫£i ph√°p:**
```python
# T√≠nh class weights
class_weights = {
    0: len(y) / (2 * count_class_0), 
    1: len(y) / (2 * count_class_1)  
}

# √Åp d·ª•ng v√†o loss function
sample_weights = np.array([class_weights[int(label)] for label in y])
weighted_loss = np.sum(sample_weights * individual_losses) / np.sum(sample_weights)
```


### Th√°ch th·ª©c 4: High Cardinality Feature (`city` = 123 values)

**V·∫•n ƒë·ªÅ:**
- One-hot encoding t·∫°o 123 features m·ªõi ‚Üí curse of dimensionality
- Overfitting risk cao

**Gi·∫£i ph√°p:**
- **Lo·∫°i b·ªè** `city` feature
- **Gi·ªØ l·∫°i** `city_development_index` (ƒë√£ capture th√¥ng tin v·ªÅ th√†nh ph·ªë)

### Th√°ch th·ª©c 5: Missing Values nhi·ªÅu (>30%)

**V·∫•n ƒë·ªÅ:**
- `company_size`: 30.9% missing
- `company_type`: 32.0% missing
- Kh√¥ng th·ªÉ b·ªè samples (m·∫•t qu√° nhi·ªÅu d·ªØ li·ªáu)

**Gi·∫£i ph√°p:**
1. **Ordinal encoding**
2. **Replace b·∫±ng mode**
3. **Feature interactions**

---

## H∆∞·ªõng ph√°t tri·ªÉn

### C·∫£i thi·ªán Model
 **Advanced Feature Engineering**
   - Polynomial features (degree 2)
   - Target encoding cho `city` (with cross-validation)
   - Binning cho `training_hours`


## Contributors

 **Tr·∫ßn T·∫° Quang Minh** - 23122042
 **email: ttqminh2005@gmail.com, github: coutMinh**

---


---

## License

### Dataset License

The original dataset used in this project is from [HR Analytics: Job Change of Data Scientists](https://github.com/josumsc/hr-analytics-ds) and is licensed under the **GNU General Public License v3.0**.

[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)

---

